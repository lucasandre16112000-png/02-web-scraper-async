"""
Exemplo de uso do Web Scraper com URLs customizadas
Execute este arquivo para testar o scraper com diferentes URLs
"""

import asyncio
from scraper import WebScraper
import json
from datetime import datetime
from dataclasses import asdict


async def main():
    """Exemplo com URLs customizadas"""
    
    # Voc√™ pode customizar estas URLs com qualquer site p√∫blico
    urls = [
        "https://news.ycombinator.com",
        "https://www.reddit.com/r/programming",
        "https://www.techcrunch.com",
    ]
    
    print("=" * 80)
    print("WEB SCRAPER ASS√çNCRONO - EXEMPLO CUSTOMIZ√ÅVEL")
    print("=" * 80)
    
    # Configurar o scraper com seus pr√≥prios par√¢metros
    scraper = WebScraper(
        requests_per_second=2.0,  # M√°ximo de 2 requisi√ß√µes por segundo
        timeout=10,                # Timeout de 10 segundos
        max_retries=3              # M√°ximo de 3 tentativas por URL
    )
    
    print(f"\nüìä Iniciando scraping de {len(urls)} URLs...")
    print(f"‚è±Ô∏è  Rate limit: 2 requisi√ß√µes/segundo")
    print(f"üîÑ M√°ximo de tentativas: 3")
    print(f"‚è≥ Timeout: 10 segundos\n")
    
    # Executar o scraping
    articles = await scraper.scrape_articles(urls)
    
    # Exibir resultados
    print("\n" + "=" * 80)
    print("RESULTADOS DO SCRAPING")
    print("=" * 80)
    
    if articles:
        for i, article in enumerate(articles, 1):
            print(f"\nüìÑ Artigo {i}:")
            print(f"   T√≠tulo: {article.title[:70]}...")
            print(f"   URL: {article.url}")
            print(f"   Autor: {article.author or 'N√£o dispon√≠vel'}")
            print(f"   Data: {article.published_date or 'N√£o dispon√≠vel'}")
            print(f"   Resumo: {(article.summary or 'N√£o dispon√≠vel')[:70]}...")
    else:
        print("\n‚ö†Ô∏è  Nenhum artigo foi extra√≠do com sucesso.")
    
    # Exibir estat√≠sticas
    stats = scraper.get_stats()
    print("\n" + "=" * 80)
    print("ESTAT√çSTICAS")
    print("=" * 80)
    print(f"Total de URLs processadas: {stats['total_items']}")
    print(f"Sucessos: {stats['successful_items']}")
    print(f"Falhas: {stats['failed_items']}")
    print(f"Tempo total: {stats['total_time']:.2f} segundos")
    print(f"Taxa m√©dia: {stats['items_per_second']:.2f} itens/segundo")
    print(f"Status: {stats['status'].value}")
    
    # Salvar resultados em JSON
    # Converter stats para dicion√°rio e serializar o status
    stats_dict = dict(stats)
    stats_dict['status'] = stats_dict['status'].value
    
    output = {
        "timestamp": datetime.utcnow().isoformat(),
        "articles": [asdict(a) for a in articles],
        "statistics": stats_dict
    }
    
    with open("scraping_results.json", "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ Resultados salvos em 'scraping_results.json'")
    print("\nüí° Dica: Abra o arquivo 'scraping_results.json' para ver os dados em formato JSON")


if __name__ == "__main__":
    asyncio.run(main())
