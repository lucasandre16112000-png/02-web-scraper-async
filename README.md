# üï∑Ô∏è Web Scraper Ass√≠ncrono Profissional

Um web scraper de alta performance constru√≠do com Python, `asyncio` e `aiohttp`. Projetado para extrair dados de m√∫ltiplos sites em paralelo, com rate limiting inteligente, retry autom√°tico e tratamento robusto de erros.

## ‚ú® Funcionalidades Principais

- **Processamento Ass√≠ncrono**: Utiliza `asyncio` e `aiohttp` para fazer m√∫ltiplas requisi√ß√µes HTTP em paralelo, aumentando drasticamente a velocidade de coleta de dados.
- **Rate Limiting Inteligente**: Inclui uma classe `RateLimiter` para controlar a frequ√™ncia das requisi√ß√µes, evitando sobrecarregar o servidor de destino e ser bloqueado.
- **Retry Autom√°tico com Exponential Backoff**: Tenta novamente requisi√ß√µes que falharam (ex: por timeout ou erro de rede) com um tempo de espera que aumenta exponencialmente, melhorando a resili√™ncia do scraper.
- **Logging Detalhado**: Fornece feedback em tempo real sobre o progresso do scraping, incluindo sucessos, avisos e erros.
- **Extra√ß√£o Estruturada**: Extrai dados estruturados (t√≠tulo, autor, data, resumo) de p√°ginas HTML usando BeautifulSoup.
- **Estat√≠sticas Completas**: Calcula e exibe estat√≠sticas detalhadas como taxa de sucesso, tempo total e velocidade m√©dia.
- **Exporta√ß√£o em JSON**: Salva automaticamente todos os resultados em um arquivo JSON bem formatado.

## üõ†Ô∏è Tecnologias Utilizadas

| Tecnologia | Vers√£o | Prop√≥sito |
|:---|:---|:---|
| **Python** | 3.8+ | Linguagem principal |
| **aiohttp** | 3.9.1 | Cliente/Servidor HTTP ass√≠ncrono |
| **BeautifulSoup4** | 4.12.2 | Parsing de HTML e XML |
| **lxml** | 4.9.3 | Parser XML/HTML de alta performance |

## üìÇ Estrutura do Projeto

```
/02-web-scraper-async
‚îú‚îÄ‚îÄ scraper.py              # C√≥digo principal do scraper
‚îú‚îÄ‚îÄ example_urls.py         # Exemplo de uso com URLs customizadas
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ .env.example           # Exemplo de arquivo de configura√ß√£o
‚îú‚îÄ‚îÄ .gitignore             # Arquivos a ignorar no Git
‚îî‚îÄ‚îÄ README.md              # Este arquivo
```

## üìã Guia de Instala√ß√£o e Execu√ß√£o (Para Qualquer Pessoa)

Este guia foi feito para que qualquer pessoa, mesmo sem conhecimento t√©cnico, possa executar este projeto.

### Pr√©-requisitos

1. **Git**: Ferramenta para baixar (clonar) o c√≥digo do GitHub.
   - [**Download do Git aqui**](https://git-scm.com/downloads)

2. **Python**: A linguagem de programa√ß√£o usada no projeto (vers√£o 3.8 ou superior).
   - [**Download do Python aqui**](https://www.python.org/downloads/)
   - **Importante**: Durante a instala√ß√£o do Python no Windows, marque a caixa que diz **"Add Python to PATH"**.

### Passo 1: Baixar o Projeto (Clonar)

Abra o seu terminal (ou **Git Bash** no Windows) e use o comando abaixo para baixar o projeto:

```bash
git clone https://github.com/lucasandre16112000-png/02-web-scraper-async.git
cd 02-web-scraper-async
```

### Passo 2: Criar e Ativar um Ambiente Virtual

Um ambiente virtual isola as depend√™ncias do projeto, evitando conflitos com outras aplica√ß√µes Python.

**No Windows (PowerShell):**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**No macOS ou Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

Voc√™ saber√° que o ambiente virtual est√° ativado quando ver `(venv)` no in√≠cio da linha do seu terminal.

### Passo 3: Instalar as Depend√™ncias

Com o ambiente virtual ativado, instale as bibliotecas necess√°rias:

```bash
pip install -r requirements.txt
```

### Passo 4: Executar o Scraper

Execute o script principal para come√ßar o scraping:

```bash
python scraper.py
```

### Passo 5: Verificar os Resultados

- O terminal mostrar√° o progresso do scraping em tempo real com emojis e mensagens claras.
- Ao final, um arquivo chamado `scraping_results.json` ser√° criado na mesma pasta, contendo todos os dados extra√≠dos em formato JSON.
- Voc√™ pode abrir este arquivo com qualquer editor de texto ou visualizador JSON.

## üöÄ Exemplos de Uso

### Exemplo 1: Usar o Script Padr√£o

O script padr√£o (`scraper.py`) j√° cont√©m um exemplo pronto para usar:

```bash
python scraper.py
```

### Exemplo 2: Customizar URLs

Para scraper URLs diferentes, edite o arquivo `example_urls.py` e modifique a lista `urls`:

```python
urls = [
    "https://seu-site-1.com",
    "https://seu-site-2.com",
    "https://seu-site-3.com",
]
```

Depois execute:

```bash
python example_urls.py
```

### Exemplo 3: Usar o Scraper em Seu Pr√≥prio C√≥digo

Voc√™ pode importar o scraper em seu pr√≥prio projeto Python:

```python
import asyncio
from scraper import WebScraper

async def meu_scraper():
    scraper = WebScraper(
        requests_per_second=2.0,  # M√°ximo de 2 requisi√ß√µes por segundo
        timeout=10,                # Timeout de 10 segundos
        max_retries=3              # M√°ximo de 3 tentativas
    )
    
    urls = ["https://exemplo.com", "https://outro-site.com"]
    articles = await scraper.scrape_articles(urls)
    
    for article in articles:
        print(f"T√≠tulo: {article.title}")
        print(f"URL: {article.url}")

asyncio.run(meu_scraper())
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Par√¢metros do WebScraper

Ao criar uma inst√¢ncia do `WebScraper`, voc√™ pode customizar os seguintes par√¢metros:

```python
scraper = WebScraper(
    requests_per_second=2.0,  # Taxa de requisi√ß√µes (padr√£o: 2.0)
    timeout=10,                # Timeout em segundos (padr√£o: 10)
    max_retries=3              # M√°ximo de tentativas (padr√£o: 3)
)
```

- **requests_per_second**: Controla quantas requisi√ß√µes s√£o feitas por segundo. Valores menores s√£o mais respeitosos com o servidor.
- **timeout**: Tempo m√°ximo de espera para cada requisi√ß√£o em segundos.
- **max_retries**: N√∫mero de tentativas antes de desistir de uma URL.

## ü§î Solu√ß√£o de Problemas Comuns

### Problema: "ModuleNotFoundError: No module named 'aiohttp'"

**Solu√ß√£o**: Certifique-se de que:
1. O ambiente virtual (venv) est√° ativado (voc√™ deve ver `(venv)` no terminal)
2. Voc√™ executou `pip install -r requirements.txt`

### Problema: "Erros de Conex√£o ou Timeout"

**Solu√ß√£o**: 
- A internet pode estar inst√°vel ou o site alvo pode estar bloqueando requisi√ß√µes.
- Tente aumentar o `timeout` ou reduzir `requests_per_second`.
- O script j√° tenta lidar com isso automaticamente, mas se o erro persistir, pode ser um problema de rede.

### Problema: "SSL: CERTIFICATE_VERIFY_FAILED"

**Solu√ß√£o**: O script j√° desativa a verifica√ß√£o SSL por padr√£o. Se o erro persistir, tente:
```python
# Adicione isto ao c√≥digo
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```

### Problema: "Arquivo scraping_results.json n√£o foi criado"

**Solu√ß√£o**:
- Verifique se o script executou at√© o final sem erros.
- Certifique-se de que voc√™ tem permiss√£o de escrita na pasta do projeto.
- Verifique o terminal para ver se h√° mensagens de erro.

## üìä Entendendo a Sa√≠da

Quando voc√™ executa o scraper, voc√™ ver√° uma sa√≠da como esta:

```
================================================================================
WEB SCRAPER PROFISSIONAL - EXEMPLO DE USO
================================================================================

üìä Iniciando scraping de 3 URLs...
‚è±Ô∏è  Rate limit: 2 requisi√ß√µes/segundo
üîÑ M√°ximo de tentativas: 3

‚úì Fetched: https://news.ycombinator.com
‚úì Fetched: https://www.reddit.com/r/programming
‚úì Fetched: https://www.techcrunch.com

================================================================================
RESULTADOS
================================================================================

üìÑ Artigo 1:
   T√≠tulo: Hacker News
   URL: https://news.ycombinator.com
   Autor: N/A
   Data: N/A
   Resumo: N/A

[... mais artigos ...]

================================================================================
ESTAT√çSTICAS
================================================================================

Total de URLs: 3
Sucesso: 3
Falhas: 0
Tempo total: 5.23s
Taxa m√©dia: 0.57 itens/segundo
Status: completed

‚úÖ Resultados salvos em 'scraping_results.json'
```

## üìÅ Formato do Arquivo JSON de Sa√≠da

O arquivo `scraping_results.json` cont√©m todos os dados extra√≠dos em um formato estruturado:

```json
{
  "timestamp": "2025-12-12T18:30:45.123456",
  "articles": [
    {
      "title": "Hacker News",
      "url": "https://news.ycombinator.com",
      "author": null,
      "published_date": null,
      "summary": null,
      "scraped_at": "2025-12-12T18:30:45.123456"
    }
  ],
  "statistics": {
    "total_items": 3,
    "successful_items": 3,
    "failed_items": 0,
    "total_time": 5.23,
    "items_per_second": 0.57,
    "status": "completed"
  }
}
```

## üîí Boas Pr√°ticas e √âtica

- **Respeite o robots.txt**: Sempre verifique o arquivo `robots.txt` do site antes de fazer scraping.
- **Use Rate Limiting**: N√£o fa√ßa requisi√ß√µes muito r√°pidas para n√£o sobrecarregar os servidores.
- **Verifique os Termos de Servi√ßo**: Certifique-se de que voc√™ tem permiss√£o para fazer scraping do site.
- **Identifique-se**: Use um User-Agent apropriado (o script j√° faz isso automaticamente).
- **N√£o Armazene Dados Pessoais**: Tenha cuidado ao coletar dados que possam conter informa√ß√µes pessoais.

## üë®‚Äçüíª Autor

Lucas Andr√© S - [GitHub](https://github.com/lucasandre16112000-png)

## üìù Licen√ßa

Este projeto √© licenciado sob a MIT License. Veja o arquivo LICENSE para mais detalhes.
